Gradient descent

Great, now that we know about the error function we want to minimize, let's talk about the general algorithm known as gradient descent. This is a training method to minimize the error of our neural network. It consists on adjusting the weights in order to find the minimum error. To get an idea of the logic behind gradient descent, think about trying to go downhill to the lowest valley in the error function. 

So here's a graphic aid to understand what gradient descent is all about. For now, suppose we have a neural network with several weights, but we are only interested in modifying one weight to see how it affects the overall error of the network. So let's say we get this function, and we set this weight to say this value. Now think of valuating this function as placing a marble over the function like this. 

Reducing the weight will get us to a higher thus worst error but increasing the weight will lead us downhill. If we keep increasing the weight, we'll eventually reach the lowest point in the plot. That's our objective. This is the global minimum of the function and that's the best error we can get by modifying this particular weight. So in some sense, we want to simulate gravity in this plot and that's what gradient descent will do for us. Now, let me quickly warn you about a possible problem here. 

Since we don't know what our error function will look like, and we initialize our weights randomly, what would happen if we start with this value for the weight? Well, simulating the same gravity effect with gradient descent, we would move the marble to the left, eventually getting stuck in this valley, which is not the global minimum, but a local minimum. Don't worry, there are several methods to overcome this local minima problem. Pushing our example a little forward, think about modifying two weights to manipulate the error. This would give us a 3D plot where the height is the error, and the two weights will place the marble at different points in this surface with mountains and valleys. The objective is still to get the marble to the lowest point. 

As you can see, it becomes a bit more complicated with more weights. So with two weights, this became a tri-dimensional plot. Well, the (indistinct) neural network has nine weights, so this becomes a 10 dimensional plot, which we can't even understand graphically. The good news is that we have a method that will simulate this gravity for us, so we don't have to worry about the number of weights involved.