Multilayer perceptrons

Now we've come to the point when we'll talk about the multilayer perceptron. This is the best-known feedforward neural network and it's one of the oldest models of the brain. It consists of neurons organized in layers and the data traverses the network from input to output. 

This is typically sketched from left to right. Feedforward neural networks have a so-called fully connected architecture between layers of neurons. Here's an example. First, we have the input layer. It contains the inputs of the network, technically known as the input vector. This is the only layer that does not contain neurons. You can think of these elements as input terminals. This is important, especially for the implementation, so I'll say it again. You don't have to place neurons in this layer. Second, we have the set of hidden layers, which are composed of neurons. Notice how all neurons from the first hidden layer are taking in all of the inputs from the input layer. 

The second hidden layer is composed of neurons that take all of the outputs from the first hidden layer. This is what fully connected means in this context. These layers are called hidden because the neural network does not expose them to the outside world. The outside world modifies the behavior of the network through its inputs, and the outside world gets modified by the network through its outputs, but the neurons in the middle layers are hidden. 

Lastly, we have the output layer, which is the last layer of neurons. There are as many neurons in this layer as outputs in the network. So there you have it. Now we are ready to start writing some code.