Linear separability

Let me tell you what it is. Looking back at the pass or fail example, this situation is suitable for a perceptron because it is linearly separable. Simply put, linear separability is a property of a data set with two categories where a linear function can separate the categories. This is a 2D plot, and there is a straight line that separates the categories. Therefore, this example is linearly separable, and a perceptron can take care of it. 

For more dimensions, a plane or hyperplane would separate the categories, but it's still a linear function. Let's look at a pesky logic gate example. The XOR gate outputs one when only one of its two inputs is one, but not both. Look at the 2D plot. I dare you to find one single straight line that divides the zeros and the ones. Did you find it? Of course not because there is none. The XOR problem is not linearly separable, so it's impossible to solve it with a single perceptron. See what I did there? I chose my words carefully. 

We cannot implement an XOR gate with one perceptron, but we can implement it with three perceptrons. Let me show you how. Recall the OR gate. This gate takes care of the first three XOR cases in the truth table. Look at the 2D plot of the XOR gate. If we used an OR gate, we would get all but one of the points correctly classified. The top right point should be zero, but the OR gate would classify it as one. What about a NAND gate? This one gets the last three XOR cases right. If we used a NAND gate to classify the XOR cases, we would misclassify the bottom left point. See where I'm going? If we could somehow combine the OR gate with the NAND gate, we would get an XOR. Well, almost. We still need some simple logic. 

Look at the overlap between the 2D plots. We want to classify as one only the points that get classified as one by both gates. So the AND gate would do the trick. This isn't anything new though. This is a well-known implementation of the XOR gate, a composition of an OR, a NAND, and an AND gate. Here's the schematic of such composition. So this looks like a plan. Let's create a network of three perceptrons. We've already figured out the weights for the AND and OR gates. All that's left is the NAND gate. 

So let me show you our first neural network to create this logic circuit. Here's our NAND gate. You may want to pause the video and look at the weights to verify that it indeed behaves as a NAND gate. Here is the OR gate you already designed. And finally, we plug the outputs into our AND gate. Notice that the bias is always one. And that's a very common practice to tie all the bias inputs to a constant value of one, and only deal with the bias weights.