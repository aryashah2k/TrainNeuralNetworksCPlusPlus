The training process

When training your neural network, there's a usual top level procedure I'll briefly describe to you. But first let me tell you what a dataset is. First of all, a dataset is a collection of samples that contain features and labels. We usually represent features with X and labels with Y. The input signals for the network are known as features because what you usually feed a narrow network are features of a data point, which can be represented as numbers. 

For example, length, height, price, salary number of rooms in a house, blood sugar level and so on. The labels on the other hand are the known category attached to each sample. This is how we teach the network, we show the samples to it. And finally, the network is able to learn with each feature labeled pair. So here's the usual training process. You typically want to use three datasets, a training set, a validation set and at testing set. 

The training set is used to train the network so that it learns all is supposed to learn. This is important so I'll say it again with different words. This is the only dataset that will be used with the training algorithm. The other two are used for two rounds of assessment. So we run the training set with the training algorithm. Another important detail is that we have to run the training set lots of times. Each time we run the training set with the learning algorithm is known as a Training Epoch. Well, the training process usually involves hundreds or thousands of training epochs. 

We stop after some number of epochs or until some error metric drops under a desired value. We'll talk about this error metric in a bit. After that the neural network will have learned something from the samples. The validation set is used to assess how well our neural network has learned as compared to other competitors. There's nothing we intend to do to improve the learning. Actually what's usually done here is that we prepare several classifiers, say to support vector machines and three neural networks. We may tweak our network you know, we could use one with one hidden layer and another with three hidden layers. We may also vary the number of neurons per layer and we may use different activation functions for example. 

So we train all of the competing classifiers and then we feed the validation set to all classifiers. This time we only run the dataset, we don't train the classifiers. The validation set will allow us to rank our classifiers and choose the one that shows the best performance for us. Let's say that's our neural network A. Lastly, the testing set is used for evaluating the finally chosen model just to make sure it's being able to classify data it hasn't seen before. Okay, so what happens when we run one single training sample? 

This is important for us to write our training code. So first we feed an input sample X to the network then we compare the output to the correct value Y. With this output and the expected correct value we may calculate the error and we can use this error to adjust the weights in the network. And we do that to classify that sample a little better in the future without messing up our response to other samples the network learn earlier.