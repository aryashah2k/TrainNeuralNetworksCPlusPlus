The need for training

It's time to talk about the need for training. So let's look back and reflect on the following points. Throughout the coding exercises of this course, we haven't seen a useful neural network yet. True, we have seen networks that behave like gates and they may be useful, but there are much better hard-coded alternatives to perceptions when it comes to implementing a (indistinct). 

For example, we could simply use the logic embedded in programming languages to get away with logic. Well, it turns out that the real value of neural networks lie in their ability to learn. Sure, we just got a multi-layer perceptron to behave as a next or gate by writing the exact weights it needed. But what if we could show the neural network a lot of examples of how (murmurs) behaves so that it can learn from those examples? Wouldn't that be something? So I have good news upper ahead we'll see an algorithm to train multi-layer perceptrons known as the backpropagation algorithm. 

So sit tight and pay attention, but wait there's another reason to train a neural network. Remember linear separability? Well, I have bad news. Linear separability is hardly a given considered this example of classifying things as small or large based on their length and width. Let's say that small is represented by triangles and large is represented by dots. Moreover, notice that this is not linearly separable. 

There is no straight line that will divide the two categories, but that doesn't mean that a single perceptron won't do a good job at classifying these samples. Take this line for example, notice that it will misclassify one dot and two triangles, which doesn't seem so bad. And even if we use the multi-layer perceptron we will get a nonlinear boundary like this one which does a better job. Misclassifying, only one triangle it's better but it's not perfect. And that's the whole point of training. We are looking for a model that will get most of the samples correctly classified because we don't know so much about the problem. 

And we are basing our judgment on the samples we have seen. This brings me to the problem of generalizing. This neural network may work very well for the provided data points but it still has to prove useful for new data. it hasn't seen before. So let me tell you about three situations in the spectrum of misclassifying and generalizing. Here we have a different data set for the same problem we just saw. I'm showing you the same blood three times because I want you to compare these three situations for a classifier. 

So look at the leftmost blot and suppose we use a single perceptron with a straight line as boundary, as you can see this one misclassified two dots and five triangles. This situation is known as underfitting where the network misclassified too often. So it's not very accurate. So we don't want this, this is bad. Now look at the middle plot. Suppose we use a multi-layer perceptron that ends up using an arc as a boundary. Notice that the misclassification has dropped to one dot and two triangles. 

Now these numbers aren't as important as the visible shape of the trend between the categories. When we train a neural network we are aiming for a boundary that works just right. That is it rarely misclassified. And it generalizes well, if we feed new unseen samples to this network, chances are, it will get it right most of the time. Now look at the right most blot, suppose we exhaustively train a very complex neural network so that it always gets it right with a perfectly accurate boundary. 

Notice that it seems wrong to get every outlier correctly classified. It seems wrong because it is wrong. This situation is known as overfitting and you may have guessed that an overfitting neural network is bad at generalizing. If we feed new on-scene data to this classifier it will probably fail often for data points near the boundary. In the real world, outliers are inevitable and we don't need to sacrifice the accuracy of our classifier just to classify known data.