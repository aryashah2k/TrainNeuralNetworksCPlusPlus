Types of classifiers

So let me show you a few classifiers in machine learning and where neural networks fit in. First, we have the logistic regression, which is a function with an input vector and a single return value. Depending on the values in the input vector, the function will return a value between zero and one as a way of classifying the input as belonging to class zero or class one. This classifier is technically a very simplistic neural network known as a perceptron. We'll take a detailed look at perceptrons shortly. 

Another type of classifier is the K-nearest neighbors algorithm. To understand how it works, suppose that we plot our samples in a 2D plane so that we have some samples that belong to one category shown as squares, and some other samples belonging to a second category shown as triangles. This algorithm takes in a new sample of an unknown category shown as a circle near the center and classifies it in the same category as the majority of its K-nearest neighbors. To avoid ties, K must be an odd number. 

And even so, its value is very crucial. For instance, suppose we chose K equals three for this plot. Then the new sample would be classified as a triangle because most of its neighbors are triangles. However, if K equals five, then it would fall into the square category. That said, the advantage of this algorithm is its evident simplicity. There are very few calculations going on. 

Support vector machines are very similar to neural networks in their input and output structure, and they often solve the same problem. Take this plot for example. A support vector machine is capable of finding the line that acts as a boundary between the two categories of points shown as squares and triangles respectively. This boundary is a line for this 2D example, but in general, it's known as a hyperplane. So here we have a valid boundary with a potential inconvenience. 

It's too close to some samples. Here's another boundary that's way too close to some samples. One of the most useful features of support vector machines is that they are capable of finding something close to the optimal boundary, that is a line that separates the categories while maximizing its distance to every point in the plot. Here's an example of such boundary. A seemingly different approach to classification are decision trees. These algorithms have a tree-like structure of questions very much like a series of if-else statements, but not quite. It turns out that a decision tree uses training algorithm based on information theory to produce the shortest possible tree. That is, to classify a new sample in the smallest number of steps. 

In the illustration, we have a decision tree to determine whether a passenger of the Titanic survived or not. This tree was produced by observing data and noticing that most of the survivors in the Titanic were women and young children with not so many siblings. It's not perfect. It may fail sometimes because it's based on probabilities, but the goal is to come up with a decision tree that asks fewer questions, while getting it right more often. The tree is very simple, and any programmer could write the code for this decision tree. 

The really tricky part is getting a computer to figure that out from the survivor data. And finally we have the classifier we are interested in, feedforward neural networks. Here are some features of neural networks that make them stand out among other tools. They were biologically inspired to mimic the brain. So although they end up performing like other tools, their architecture is very flexible. They may have as many outputs as needed in a single network, and their learning algorithm is very clever and simple.