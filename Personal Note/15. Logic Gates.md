Logic gates


In their early days, neural networks were tested with simple functions to see if they were capable of performing the calculations they were designed to perform. This led to implementing logic gates with perceptrons. So let's take a look at a two input AND gate. Here we have its truth table, which summarizes its behavior. 

Supposing zero means false and one means true, the gate outputs true only in the case where both inputs are true. You may already be familiar with all of this, but let's look at it from a totally different angle, as a classification problem. This plot shows four data points. The coordinates of these data points are the values of inputs A and B. Notice that the data points show their category as a zero or a one. This way, a two input classifier may come up with a boundary that divides the categories. 

Based on this, a perceptron may behave as an AND gate. So let me show you how the classification is possible. The boundary we are seeing in this 2D plot is technically the line where the sigmoid is 0.5. So a 3D plot may help making this explanation better. So let's add a third axis now vertical to express the output of the sigmoid. This will be the category that the perceptron has inferred. 

So if we apply the sigmoid, all the samples will be a part of the surface of the sigmoid, like objects lying on this uneven terrain. So once again, the height of these objects finally determines the category the perceptron assigns to them. The decision plane will be located at the middle of the sigmoid. So here's a valid implementation of a two input AND gate. It's a two input perceptron with the following weights: 10 for both inputs, and -15 for the bias. 

I just came up with these weights by aiming to get a negative sum for our result of zero, and a positive sum for one. After testing it with my perceptron, I get the values shown in this table. Notice that I included a column for the weighted sum z. Those were the values I was aiming for with the weights I used. In the Y column, the three top values are very close to zero, so they may be safely rounded to zero, and the last value is almost one. So here's how I tested the perceptron in the code. 

This is the first time we'll run some code, so we need a main function. I wrote this in the neuralnetworks.cpp file. Starting at line 12, I created a perceptron with two inputs. And then in line 14, I entered the weights as a list, which is acting as an initializer for the vector argument. Notice the order of the weights, 10, 10, and -15 for the bias. That's it. Now I'm just testing all four cases with the run function and sending it to cout. 

So let's see it working. Great. So our perceptron can indeed operate as an AND gate. Let's move on.