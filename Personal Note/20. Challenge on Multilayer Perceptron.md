Challenge: Finish the multilayer perceptron class

Now, you must write two functions, so we may test our multi-layer perceptron class. First, we have the set weights function starting at line 52. As argument, you may use the organization you want, but I suggest that you make it capable of initializing a network of any size. 

Don't forget the bias weights. In the code, you'll see that I declared the argument w_init as a vector of vectors of vectors of doubles. If you'd like to use some other structure make sure you also change the function prototype in MLP.age, or you may want to overload this function with your own, your choice. 

Now, look at line 57, I've written a print_weights function for you to check if your neural network has received the weights correctly and to see the weights when you have trained it later in the course. Second, we have the run functions starting at line 70, which feeds a sample to the network and returns a vector with the output values. I've written the return line, just to be clear. 

We simply return the last element in the values vector, which is a vector containing exactly the output layer values. So, to recap, you must write a function to write values to the weights and the run function to produce an output. You can, finally, test your new neural network with the XOR gate weights we just saw. This shouldn't take you more than 15 minutes.