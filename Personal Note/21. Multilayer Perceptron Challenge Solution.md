Solution: Finish the multilayer perceptron class

For the set_weights function in line 50, I'm implementing w_init as a vector of vectors of vectors, of doubles. That's three dimensions. And this is because I'm specifying the layer, the neuron, and the input associated to each weight. However, w_init will have one less entry in the first dimension because I'm not specifying anything for the input layer as it has no neurons. 

So I implemented two nested loops. The outer loop iterates i through the layers in the network and the inner loop iterates j through the neurons in each layer. Now inside the inner loop, I'm using the set_weights function for each neuron. Notice that since w_init doesn't have anything for the input layer, I'm indexing the network array at i + 1. Now for the run function in line 70, the first thing I do is copy x into the first layer of the values vector. Now it's time to run a two-level nested loop for every layer in ascending order, and every neuron in each layer. The body of the loop is simply running the current neuron by feeding it the values in the previous layer. That's it. Now let's test the whole thing in NeuralNetworks.cpp. 

You'll see that I left the previous examples and I'm adding this hard-coded XOR example at the end, starting at line 27. First, I created a MultiLayerPerceptron with the dimensions of the XOR gate design. Next I'm assigning the required weights. As a sanity check, I'm printing out to check that the weights were successfully assigned. And finally, here we have four printing lines to test our network with all four cases of the two inputs. Let's run it. And here's the result. As you can see, it's indeed behaving as an XOR gate. Give yourself a pat on the back.