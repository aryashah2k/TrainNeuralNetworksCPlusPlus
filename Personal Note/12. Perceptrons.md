Perceptrons: A better model of a neuron

Now we have our complete model known as a perceptron. As you can see, it has a set of inputs with a global bias term. This input vector will go through a weighted sum, and this value will go into our sigmoid of activation function. Once again, pay attention to the numbering. 

The inputs and their weights are numbered from zero to n minus one, and the bias is treated as input number n. So, how should we interpret the output values of our neuron? Well, the output comes from the sigmoid function. Notice that the output is greater than 0.5 for a positive input. 

That is, for a positive weighted sum. This way, an output value of 0.5 seems like a reasonable threshold for firing. So before we dive into the code, let me point out some implementation notes. All values must be real numbers, not integers. So I'll use double precision floating point numbers, or the double C++ type. The weights and inputs may be implemented as one dimensional vectors. 

In our case, we'll use the generic vector from the C++ standard template library. This way, the weighted sum may be calculated in one operation as the dot product between the two vectors. That's one line of code. Finally, we'll feed the sum to our implementation of the sigmoid function. So here's the code. We are looking at mlp.h, and this is the only time we'll look at this file for the perceptron class, which starts at line 12. The only member data we need for this class 