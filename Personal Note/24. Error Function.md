Error function

Earlier I mentioned the error of a neural network output, so let me tell you what I meant by that. An error function measures how bad a classifier is doing. So a large value is bad and a small value is good. This function is essential in the training process. 

So up ahead, we'll learn about a training process called gradient descent. Throughout the algorithm, we'll calculate two types of error metrics, one for the output of a neuron and one for the whole network. So the first metric is not formally known as the error, because it's just the measure of how far off a neuron is from the expected value dictated by the label in the dataset. It's simply a subtraction. Let me show you. 

Suppose we enter a sample x,y to a neural network which for now will be a single layer perceptron. Now suppose that the output of this one output network is 0.6. And let's say that the label for that input sample is y equals one. This way, the error or deviation can be calculated as the subtraction y minus out. Again, this output error is just a step in the calculations, but it will show up quite often. So the point is that the training function must somehow contribute to getting out closer to y over time, that is, making the error approach zero. 

Now, when we are assessing the performance of the neural network, we use a metric known as the mean squared error. There are several important details about this function. The error is calculated as the sum of the squared output errors for all neurons in the output layer, all of this divided by n, the number of neurons in the output layer. Remember a neural network may have more than one outlet. 

Since we'll use this error to check how our training is going, the training process seeks to minimize this error. A nice thing about this metric is that it gets rid of the sign of the actual error. So when minimizing the error, we're not interested in the direction of this error. It's all the same to us if the output is over or under the desired value. What we extract from this function is the size of the error. This way, we always want to minimize this function.