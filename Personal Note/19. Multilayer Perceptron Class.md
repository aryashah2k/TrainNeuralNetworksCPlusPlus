Writing the multilayer perceptron class

So it turns out that the network we just described is a multilayer perceptron. Think about it. It's made of perceptrons, they are interconnected in a feed-forward fashion and they are organized in layers. This is the input layer. Remember, no neurons, just the inputs. This is the hidden layer with two neurons and this is the output layer with just one neuron. So it's time to start writing code again. 

So here's our first implementation of the multilayer perceptron class. I'll write the constructor and you'll write the rest. But first, let's look at MLP.h to see the class members starting at line 30. First, we have layers, which is a vector of integers that represent the number of neurons per layer. This includes the input layer, which has no neurons but here we mean the number of inputs. The bias is also a member. There's a third member called eta in line 32, which is known as the learning rate. 

We'll talk about it later. Next, we have the actual network. It's a vector of vectors of perceptrons. Then we'll need another vector with the same dimensions as the network to hold the output values of the neurons. I named it values. This will be useful for propagating the results forward through the network. Finally, we have yet another vector of vectors of doubles called d. It will contain the so-called error terms for the neurons. We'll learn about this later. So let's go to MLP.cpp. Look at the constructor starting at line 34. It has three parameters: the layers vector, the bias with the usual default value of one and eta with a default value of 0.5. 

Remember, we'll use this last parameter later. So in lines 35 through 37, the members are initialized with the arguments. The next part consists of two nested loops to create the neurons layer by layer. The outer loop iterates on i for each layer. So for each layer, we need to add a vector of values and a vector of neurons. The new vector of values will be filled with zeros for every neuron in the layer. And the vector of neurons will be empty for now. Now, the inner loop iterates on j for each neuron in the layer but will leave the first layer empty because it has no neurons. 

So for every neuron, I'll create a perceptron with as many inputs as the neurons in the previous layer. Remember, the bias input doesn't count here. And I also pass the bias value to the perceptron constructor so that it's it for the constructor.