The delta rule

The simplest form of the algorithm we'll implement is known as the delta rule. It's a simple update formula for adjusting the weights in a single perceptron, that is a neuron. Yes, it's a simple formula, but its reasoning is very clever. The delta rule considers the following values; the output error, this is the simple subtraction error, one input, the one effected by the weight we are going to tweak, and a constant known as the learning rate. So here's a nice equation to calculate the update in a weight I in a neuron K. 

Let's call it delta W sub I K. And it's the value we'll have to add to W sub I K to get the boundary closer to what we want. So to calculate this delta, we multiply the learning rate times the output error that is the label Y sub K, minus the neurons output O sub K, times the I-th input value X sub I K. Yeah, it's very simple. But notice what's happening there. The output error will be positive if the output is higher than the desired output, and it will be negative if the output is lower than the label. This means that when we later update W, it will contribute to making the output closer to the provided label. If we calculate all of the delta W's and add them to the W's, our perception will be one step closer to having the boundary we want. 

So let me tell you a few things about the learning rate. First, it's a unique constant in the neural network. There's only one learning rate for all neurons. As the name suggests, it directly affects the rate of learning because higher values will result in larger leaps for the weights and lower values will result in smaller leaps for the weights. Does a higher learning rate mean faster learning? Yes. Does a higher learning rate mean better? No. The learning rate is usually initialized at 0.5 but you may have to tune it if learning is too fast or too slow. Here's why. Let me show you six updates of a weight in this error function considering a learning rate that's too slow. So pay attention to the marble. One, two, three, four, five, six. The marble will eventually find a minimum. It will take a long time and it may get stuck at the first local minimum it finds. 

This could be much better. Let's see six steps again with a learning rate that's too high. One, Two, three, four, five, six. Large leaps may miss the minimum, getting stuck around it or even missing it altogether. The desired situation is a learning rate that's just right. Notice that it may mimic inertia going a bit past the minimum, but being drawn back into the valley. One, two, three, four, five, six.