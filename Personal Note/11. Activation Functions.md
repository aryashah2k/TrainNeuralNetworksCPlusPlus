Activation functions

We are almost there, but our neuron is still missing something. So let me tell you what's wrong with weighted sums. There are two inconveniences I'd like to mention. First, values aren't constrained, so as sum may sometimes result in a very large value or a very small value. 

Second, a weighted sum is a linear function, so the threshold to "fire" is not very well-defined. That is, a change between true and false is not very notable, and most importantly, it's not easily trained. It turns out that other functions that make learning easier are nonlinear. This is the real reason to add an element to our neuron. So what's wrong with having a very large and a very small value? 

Considered this example where we have a two input neuron, and we are feeding 1,000 x0 and two to x1. For now, let's leave the bias weight at zero, so the bias is not shown to keep the diagram simple. If we run the neuron, we'll have a result of 2,006. So notice that although the weights are very similar, two and three, the big difference in the input values has made the neuron very sensitive to x0 and insensitive to x1. 

That's the job of the weights, not of the inputs. And what's wrong with linear functions? Well, consider this neuron with one feature input and one bias input. At the right we have a plot of the output as a function of x0. Now, this is not the usual 2D plot we have seen so far. The line is not the boundary. This plot is showing the output sum z as a function of x0, so the boundary is the horizontal axis. Remember, this neuron will classify the input values 